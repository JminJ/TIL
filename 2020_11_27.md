# 공부한 것 #
-----------------------------
## 수학 ##
* 내적
## 전공 ##
* torch.dot()
* Attention
------------------
## 정리 ##
* 내적
<img src = 'images\2020_11_27_1.PNG'>   
> 내적이란 벡터를 마치 수처럼 곱한다는 뜻을 가진다. 즉 벡터의 방향이 일치하는 만큼만 곱해주는 것이다. 위 그림에서 a와 b를 내적한다면 b와 방향이 같은 a의 부분만을 가져와 곱해주는 것을 볼 수 있다.
>><img src = 'images\2020_11_27_2.PNG'> 
* torch.dot()
> * torch.dot(input, tensor) => Tensor
> 두 Tensor의 dot product를 계산한다(내적)
>```python
>import torch
>
>torch.dot(torch.tensor([2,3]), torch.tensor([2,1]))
> # tensor(7)
* Attention
> Attention은 RNN의 고질적인 문제인 long-term-dependency problem과 Vanishing Gradient 문제를 해결하기 위해 고안되었다.

> <img src = 'images\2020_11_27_3.PNG'>
> * 맨위의 그래프는 attention을 적용, 나머지는 적용하지 않음
  
>  위 그림처럼 성능을 보존 시키는 attention이란 무엇일까?
>  <img src = 'images\2020_11_27_4.PNG'>
> Attention모델은 디코더 부분의 input으로 인코더에서 나온 모든 hidden state값과 바로 전 단계의 디코더 hidden state를 fully connected layer를 통해 attention score라고 불리는 값을 뽑아내고, 이 값들을 softmax시켜줘 attention weight를 구한다.  
>   
> 이 값울 디코더의 input과 함께 넣어줌으로써 어느 단어를 해당 step에서 중요하게 여길지 파악할 수 있고 long-term-dependency problem과 Vanishing Gradient 문제를 해결할 수 있다.

* 출처 : <https://www.youtube.com/watch?v=WsQLdu2JMgI>
* 출처 : <https://wikidocs.net/22384>
-------------------------------
## Notion Link ##
* torch.dot() : 
<https://www.notion.so/torch-dot-a05b4607e8824b34ab2e5701a28fd5f3>
* Attention : 
<https://www.notion.so/Attention-adc80c51ad7f4e36a73174d339c853ae>
