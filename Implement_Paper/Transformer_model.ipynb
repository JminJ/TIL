{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ-K74s-Ck-G"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO58oVuKDR-_"
      },
      "source": [
        "import torch.nn\r\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJyuL3UYDggd"
      },
      "source": [
        "##Input Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvSTyU0nDdqS"
      },
      "source": [
        "vocab = []\r\n",
        "inputs = []\r\n",
        "n_vocab = len(vocab)\r\n",
        "d_hidn = 128\r\n",
        "nn_emb = nn.Embedding(n_vocab, d_hidn) # Embedding 객체\r\n",
        "\r\n",
        "input_embs = nn_emb(inputs) # input embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6kDBC9YDtOi"
      },
      "source": [
        "## Positional Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnVB3kV2Dx05"
      },
      "source": [
        "def get_sinusoid_encoding_table(n_seq, d_hidn):\r\n",
        "    def cal_angle(position, i_hidn):\r\n",
        "        return position / np.power(10000, 2 * (i_hidn // 2) / d_hidn)\r\n",
        "    def get_posi_angle_vec(position):\r\n",
        "        return [cal_angle(position, i_hidn) for i_hidn in range(d_hidn)]\r\n",
        "\r\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\r\n",
        "    # sinusoid_table에서 0부터 2씩 증가하면서 np.sin을 취해준다.\r\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\r\n",
        "    # sinusoid_table에서 1부터 2씩 증가하면서 np.cos을 취해준다.\r\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\r\n",
        "\r\n",
        "    return sinusoid_table"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpSliNgVD-at"
      },
      "source": [
        "## Make Embedding(Input Embedding + Position Embedding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNo21h_YEJHl"
      },
      "source": [
        "n_seq = 64\r\n",
        "pos_encoding = get_sinusoid_encoding_table(n_seq, d_hidn)\r\n",
        "\r\n",
        "pos_encoding = torch.FloatTensor(pos_encoding)\r\n",
        "nn_pos = nn.Embedding.from_pretrained(pos_encoding, freeze = True)\r\n",
        "\r\n",
        "# arange를 통해 1~8까지의 tensor를 만든다.\r\n",
        "positions = torch.arange(inputs.size(1), device=inputs.device, dtype = inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous() + 1\r\n",
        "# .eq(0)을 통해 inputs에서 0인 부분을 booltensor로 만들어 낸다.\r\n",
        "pos_mask = inputs.eq(0)\r\n",
        "\r\n",
        "# pos_mask에서 True인 부분을 0으로 만들어 positoins를 수정한다.\r\n",
        "positions.masked_fill_(pos_mask, 0)\r\n",
        "# positions를 from_pretrained를 통해 임베딩한다.\r\n",
        "pos_embs = nn_pos(positions)\r\n",
        "\r\n",
        "input_sums = input_embs + pos_embs\r\n",
        "\r\n",
        "d_head = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS2YI6WFEPMG"
      },
      "source": [
        "## Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY0criH2ESpZ"
      },
      "source": [
        "class ScaledDotProductAttention(nn.Module):\r\n",
        "    def __init__(self, d_head):\r\n",
        "        super().__init__()\r\n",
        "        self.scale = 1 / (d_head ** 0.5)\r\n",
        "    \r\n",
        "    def forward(self, Q, K, V, attn_mask):\r\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\r\n",
        "        # Q와 K를 행렬곱 연산을 시킨다.\r\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)).mul_(self.scale)\r\n",
        "        # score에서 attn_mask의 True 부분을 -1e9값으로 채워준다.(softmax에서 해당 index 결과값을 0으로 만들어주기 위해)\r\n",
        "        scores.masked_fill_(attn_mask, -1e9)\r\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\r\n",
        "        attn_prob = nn.Softmax(dim=-1)(scores)\r\n",
        "        # (bs, n_head, n_q_seq, d_v)\r\n",
        "        # softmax를 거친 attn_prob와 V를 행렬곱 연산을 시켜준다.\r\n",
        "        context = torch.matmul(attn_prob, V)\r\n",
        "        # (bs, n_head, n_q_seq, d_v), (bs, n_head, n_q_seq, n_v_seq)\r\n",
        "        return context, attn_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1vAomfwEWKf"
      },
      "source": [
        "## Multi head attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Sm80MDREVpk"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\r\n",
        "    def __init__(self, d_hidn, n_head, d_head):\r\n",
        "        super().__init__()\r\n",
        "        self.d_hidn = d_hidn \r\n",
        "        self.n_head = n_head \r\n",
        "        self.d_head = d_head \r\n",
        "\r\n",
        "        self.W_Q = nn.Linear(d_hidn, n_head * d_head)\r\n",
        "        self.W_K = nn.Linear(d_hidn, n_head * d_head)\r\n",
        "        self.W_V = nn.Linear(d_hidn, n_head * d_head)\r\n",
        "        self.scaled_dot_attn = ScaledDotProductAttention(d_head) # Scaled Dot Product Attention\r\n",
        "        self.linear = nn.Linear(n_head * d_head, d_hidn)\r\n",
        "    \r\n",
        "    def forward(self, Q, K, V, attn_mask):\r\n",
        "        batch_size = Q.size(0)\r\n",
        "        # (bs, n_head, n_q_seq, d_head)\r\n",
        "        # self.W_Q(Q)의 결과값을 (batch_size, -1, self.n_head, self.d_head)형태로 바꿔주고 transpose를 통해 1번째 차원과 2번째 차원의 크기를 바꿔준다.\r\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.n_head, self.d_head).transpose(1,2)\r\n",
        "        # (bs, n_head, n_k_seq, d_head)\r\n",
        "        # self.W_K(K)의 결과값을 (batch_size, -1, self.n_head, self.d_head)형태로 바꿔주고 transpose를 통해 1번째 차원과 2번째 차원의 크기를 바꿔준다.\r\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.n_head, self.d_head).transpose(1,2)\r\n",
        "        # (bs, n_head, n_v_seq, d_head)\r\n",
        "        # self.W_V(V)의 결과값을 (batch_size, -1, self.n_head, self.d_head)형태로 바꿔주고 transpose를 통해 1번째 차원과 2번째 차원의 크기를 바꿔준다.\r\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.n_head, self.d_head).transpose(1,2)\r\n",
        "\r\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\r\n",
        "        # attn_mask의 1번째 차원에 새로운 차원을 만들고 self.n_head만큼 attn_mask를 반복해 저장한다. \r\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_head, 1, 1)\r\n",
        "\r\n",
        "        # (bs, n_head, n_q_seq, d_head), (bs, n_head, n_q_seq, n_k_seq)\r\n",
        "        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)\r\n",
        "        # (bs, n_head, n_q_seq, h_head * d_head)\r\n",
        "        # context의 1번째 차원과 2번째 차원의 크기를 바꾸고 (batch_size, -1, self.n_head * self.d_head)의 크기로 바꿔준다.\r\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.n_head * self.d_head)\r\n",
        "        # (bs, n_head, n_q_seq, e_embd)\r\n",
        "        output = self.linear(context)\r\n",
        "        # (bs, n_q_seq, d_hidn), (bs, n_head, n_q_seq, n_k_seq)\r\n",
        "        return output, attn_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn4V32DlE8FV"
      },
      "source": [
        "## Attention decoder mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IP_7jNAE7jt"
      },
      "source": [
        "def get_attn_decoder_mask(seq):\r\n",
        "    # seq의 사이즈와 같은 1로 채워진 tensor를 만들고 맨 마지막 차원에 새로운 차원을 만들고 (seq.size(0), seq.size(1), seq.size(1))의 크기로 바꿔준다. \r\n",
        "    subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1))\r\n",
        "    # subsequent_mask의 대각 행렬부터 맨 마지막 행까지 0으로 바꿔준다.\r\n",
        "    subsequent_mask = subsequent_mask.triu(diagonal=1) # upper triangular part of a matrix(2-D)\r\n",
        "    return subsequent_mask\r\n",
        "### \r\n",
        "Q = input_sums\r\n",
        "K = input_sums\r\n",
        "V = input_sums\r\n",
        "\r\n",
        "# 0과 비교하면서 unsqeeze로 2차원을 추가하고 expand로 (2,8,8)크기가 되도록 반복한다.\r\n",
        "attn_pad_mask = inputs.eq(0).unsqueeze(1).expand(Q.size(0), Q.size(1), K.size(1))\r\n",
        "print(attn_pad_mask[1])\r\n",
        "attn_dec_mask = get_attn_decoder_mask(inputs)\r\n",
        "print(attn_dec_mask[1])\r\n",
        "# attn_pad_mask + attn_dec_mask와 0을 비교하면서 0보다 큰 위치에 True를, 아닌 경우에는 False를 반환한다.\r\n",
        "attn_mask = torch.gt((attn_pad_mask + attn_dec_mask), 0)\r\n",
        "print(attn_mask[1])\r\n",
        "\r\n",
        "# barch_size : 2 \r\n",
        "batch_size = Q.size(0)\r\n",
        "n_head = 2\r\n",
        "\r\n",
        "# Multi-Head Attention\r\n",
        "attention = MultiHeadAttention(d_hidn, n_head, d_head)\r\n",
        "output, attn_prob = attention(Q, K, V, attn_mask)\r\n",
        "print(output.size(), attn_prob.size())\r\n",
        "###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRHDtKb1Bfju"
      },
      "source": [
        "## Feed forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpoWIsFXBoRl"
      },
      "source": [
        "class PoswiseFeedForwardNet(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "        self.conv1 = nn.Conv1d(in_channels=self.config.d_hidn, out_channels=self.config.d_ff, kernel_size=1)\r\n",
        "        self.conv2 = nn.Conv1d(in_channels=self.config.d_ff, out_channels=self.config.d_hidn, kernel_size=1)\r\n",
        "        self.active = F.gelu\r\n",
        "        self.dropout = nn.Dropout(config.dropout)\r\n",
        "\r\n",
        "    def forward(self, inputs):\r\n",
        "        # (bs, d_ff, n_seq)\r\n",
        "        output = self.conv1(inputs.transpose(1, 2))\r\n",
        "        output = self.active(output)\r\n",
        "        # (bs, n_seq, d_hidn)\r\n",
        "        output = self.conv2(output).transpose(1, 2)\r\n",
        "        output = self.dropout(output)\r\n",
        "        # (bs, n_seq, d_hidn)\r\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1rtAKA_B4nD"
      },
      "source": [
        "## Encoder layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvkYf-WsB_Cj"
      },
      "source": [
        "class EncoderLayer(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "        self.self_attn = MultiHeadAttention(self.config)\r\n",
        "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\r\n",
        "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\r\n",
        "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\r\n",
        "    \r\n",
        "    def forward(self, inputs, attn_mask):\r\n",
        "        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\r\n",
        "        att_outputs, attn_prob = self.self_attn(inputs, inputs, inputs, attn_mask)\r\n",
        "        att_outputs = self.layer_norm1(inputs + att_outputs)\r\n",
        "        # (bs, n_enc_seq, d_hidn)\r\n",
        "        ffn_outputs = self.pos_ffn(att_outputs)\r\n",
        "        ffn_outputs = self.layer_norm2(ffn_outputs + att_outputs)\r\n",
        "        # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\r\n",
        "        return ffn_outputs, attn_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao7Nns59CBlu"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6QI2s0cCFUJ"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "        self.enc_emb = nn.Embedding(self.config.n_enc_vocab, self.config.d_hidn)\r\n",
        "        sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config.n_enc_seq + 1, self.config.d_hidn))\r\n",
        "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)\r\n",
        "\r\n",
        "        self.layers = nn.ModuleList([EncoderLayer(self.config) for _ in range(self.config.n_layer)])\r\n",
        "    \r\n",
        "    def forward(self, inputs):\r\n",
        "        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).expand(inputs.size(0), inputs.size(1)).contiguous() + 1\r\n",
        "        pos_mask = inputs.eq(self.config.i_pad)\r\n",
        "        positions.masked_fill_(pos_mask, 0)\r\n",
        "\r\n",
        "        # (bs, n_enc_seq, d_hidn)\r\n",
        "        outputs = self.enc_emb(inputs) + self.pos_emb(positions)\r\n",
        "\r\n",
        "        # (bs, n_enc_seq, n_enc_seq)\r\n",
        "        attn_mask = get_attn_pad_mask(inputs, inputs, self.config.i_pad)\r\n",
        "\r\n",
        "        attn_probs = []\r\n",
        "        for layer in self.layers:\r\n",
        "            # (bs, n_enc_seq, d_hidn), (bs, n_head, n_enc_seq, n_enc_seq)\r\n",
        "            outputs, attn_prob = layer(outputs, attn_mask)\r\n",
        "            attn_probs.append(attn_prob)\r\n",
        "        # (bs, n_enc_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\r\n",
        "        return outputs, attn_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb-661LsCIJQ"
      },
      "source": [
        "## Decoder layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52M-ceYtCL2t"
      },
      "source": [
        "class DecoderLayer(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "        self.self_attn = MultiHeadAttention(self.config)\r\n",
        "        self.layer_norm1 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\r\n",
        "        self.dec_enc_attn = MultiHeadAttention(self.config)\r\n",
        "        self.layer_norm2 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\r\n",
        "        self.pos_ffn = PoswiseFeedForwardNet(self.config)\r\n",
        "        self.layer_norm3 = nn.LayerNorm(self.config.d_hidn, eps=self.config.layer_norm_epsilon)\r\n",
        "    \r\n",
        "    def forward(self, dec_inputs, enc_outputs, self_attn_mask, dec_enc_attn_mask):\r\n",
        "        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_dec_seq)\r\n",
        "        self_att_outputs, self_attn_prob = self.self_attn(dec_inputs, dec_inputs, dec_inputs, self_attn_mask)\r\n",
        "        self_att_outputs = self.layer_norm1(dec_inputs + self_att_outputs)\r\n",
        "        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_enc_seq)\r\n",
        "        dec_enc_att_outputs, dec_enc_attn_prob = self.dec_enc_attn(self_att_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\r\n",
        "        dec_enc_att_outputs = self.layer_norm2(self_att_outputs + dec_enc_att_outputs)\r\n",
        "        # (bs, n_dec_seq, d_hidn)\r\n",
        "        ffn_outputs = self.pos_ffn(dec_enc_att_outputs)\r\n",
        "        ffn_outputs = self.layer_norm3(dec_enc_att_outputs + ffn_outputs)\r\n",
        "        # (bs, n_dec_seq, d_hidn), (bs, n_head, n_dec_seq, n_dec_seq), (bs, n_head, n_dec_seq, n_enc_seq)\r\n",
        "        return ffn_outputs, self_attn_prob, dec_enc_attn_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBpW1Bq0COQI"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8Z8alC2CSlS"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "        self.dec_emb = nn.Embedding(self.config.n_dec_vocab, self.config.d_hidn)\r\n",
        "        sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config.n_dec_seq + 1, self.config.d_hidn))\r\n",
        "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)\r\n",
        "\r\n",
        "        self.layers = nn.ModuleList([DecoderLayer(self.config) for _ in range(self.config.n_layer)])\r\n",
        "    \r\n",
        "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\r\n",
        "        positions = torch.arange(dec_inputs.size(1), device=dec_inputs.device, dtype=dec_inputs.dtype).expand(dec_inputs.size(0), dec_inputs.size(1)).contiguous() + 1\r\n",
        "        pos_mask = dec_inputs.eq(self.config.i_pad)\r\n",
        "        positions.masked_fill_(pos_mask, 0)\r\n",
        "    \r\n",
        "        # (bs, n_dec_seq, d_hidn)\r\n",
        "        dec_outputs = self.dec_emb(dec_inputs) + self.pos_emb(positions)\r\n",
        "\r\n",
        "        # (bs, n_dec_seq, n_dec_seq)\r\n",
        "        dec_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs, self.config.i_pad)\r\n",
        "        # (bs, n_dec_seq, n_dec_seq)\r\n",
        "        dec_attn_decoder_mask = get_attn_decoder_mask(dec_inputs)\r\n",
        "        # (bs, n_dec_seq, n_dec_seq)\r\n",
        "        dec_self_attn_mask = torch.gt((dec_attn_pad_mask + dec_attn_decoder_mask), 0)\r\n",
        "        # (bs, n_dec_seq, n_enc_seq)\r\n",
        "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs, self.config.i_pad)\r\n",
        "\r\n",
        "        self_attn_probs, dec_enc_attn_probs = [], []\r\n",
        "        for layer in self.layers:\r\n",
        "            # (bs, n_dec_seq, d_hidn), (bs, n_dec_seq, n_dec_seq), (bs, n_dec_seq, n_enc_seq)\r\n",
        "            dec_outputs, self_attn_prob, dec_enc_attn_prob = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\r\n",
        "            self_attn_probs.append(self_attn_prob)\r\n",
        "            dec_enc_attn_probs.append(dec_enc_attn_prob)\r\n",
        "        # (bs, n_dec_seq, d_hidn), [(bs, n_dec_seq, n_dec_seq)], [(bs, n_dec_seq, n_enc_seq)]S\r\n",
        "        return dec_outputs, self_attn_probs, dec_enc_attn_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxaFQ7apCVsj"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3MbdQJtCZT5"
      },
      "source": [
        "class Transformer(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "        self.encoder = Encoder(self.config)\r\n",
        "        self.decoder = Decoder(self.config)\r\n",
        "    \r\n",
        "    def forward(self, enc_inputs, dec_inputs):\r\n",
        "        # (bs, n_enc_seq, d_hidn), [(bs, n_head, n_enc_seq, n_enc_seq)]\r\n",
        "        enc_outputs, enc_self_attn_probs = self.encoder(enc_inputs)\r\n",
        "        # (bs, n_seq, d_hidn), [(bs, n_head, n_dec_seq, n_dec_seq)], [(bs, n_head, n_dec_seq, n_enc_seq)]\r\n",
        "        dec_outputs, dec_self_attn_probs, dec_enc_attn_probs = self.decoder(dec_inputs, enc_inputs, enc_outputs)\r\n",
        "        # (bs, n_dec_seq, n_dec_vocab), [(bs, n_head, n_enc_seq, n_enc_seq)], [(bs, n_head, n_dec_seq, n_dec_seq)], [(bs, n_head, n_dec_seq, n_enc_seq)]\r\n",
        "        return dec_outputs, enc_self_attn_probs, dec_self_attn_probs, dec_enc_attn_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyRA_Usb2Zwj"
      },
      "source": [
        "## reference : https://paul-hyun.github.io/transformer-01/\r\n",
        "**좋은 코드 정말 감사합니다 :)**"
      ]
    }
  ]
}