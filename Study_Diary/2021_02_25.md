# 공부한 것 #
### NLP ###
* Perplexity 정리
* lstm 복습 및 정리
### Implement paper ###
* gpt-1 : multi-head-attention 구현, self-dot-attention 수정

---------
### Link ###
* <https://www.notion.so/Perplexity-ca96e61a1da446a4b154d7e8cf3faee0>
* <https://www.notion.so/LSTM-14d2786e02d747e794203dd5a862269c>
* <https://github.com/JminJ/Implement_paper/blob/main/gpt_1.py>