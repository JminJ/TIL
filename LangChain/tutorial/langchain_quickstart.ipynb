{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNeY8CkNUO7B",
        "outputId": "99529c6b-dd7a-453e-d552-2d4070973559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat May  6 13:48:01 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqgeJeVFUfFf"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lajVXpfBUecL",
        "outputId": "2e19b899-9414-4bd5-880c-a6740c3e954d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.161-py3-none-any.whl (758 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m759.0/759.0 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.65.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp<4.0.0,>=3.8.3\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Collecting async-timeout<5.0.0,>=4.0.0\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, openapi-schema-pydantic, marshmallow-enum, aiosignal, dataclasses-json, aiohttp, langchain\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.7 frozenlist-1.3.3 langchain-0.0.161 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.8.0 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.6\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Yej_dI-MVHU-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"]=\"...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWB4cfKvVdCI"
      },
      "source": [
        "## LLMs: language model로부터 prediction들을 얻자.\n",
        "가장 기본적인 LangChain의 building block은 input에 대해 LLM을 불러 내는 것이다. 해당 코드의 목적은 회사가 무슨 일을 하는가에 따라 회사명을 생성해 보는 것이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-644N3RV-_0",
        "outputId": "81bf1d1d-00b9-4218-e7c2-73b9aa24e722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "★열정같이 따뜻해지는 양말★\n",
            "\n",
            "\n",
            "Fuzzy Feet Toesies.\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Random성을 가중시키기 위해 temperature를 높게 설정해본다.\n",
        "llm = OpenAI(temperature=0.9)\n",
        "\n",
        "## KO\n",
        "text = \"다채로운 양말을 만드는 회사에 어울리는 회사명은 무엇일까?\"\n",
        "print(llm(text))\n",
        "\n",
        "## EN\n",
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "print(llm(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtn69_P4W2fg"
      },
      "source": [
        "## Prompt Templates: LLMs를 위한 prompt들을 관리한다.\n",
        "일반적으로 LLM이 어플리케이션에서 사용될 떄, 유저의 input을 그대로 LLM에 넣지는 않을 것이다.\n",
        "대신에, 아마도 당신은 유저의 input을 받고 prompt를 구성한 다음 LLM에게 그것을 전달해 줄 것이다.\n",
        "\n",
        "위 예시를 업데이트 해본다면, 우리는 유저에게 회사가 하는 일만을 입력 받고 그 정보들로 prompt의 format을 맞춰주는 것을 생각해 볼 수 있겠다. 이 작업은 LangChain을 통해서면 매우 쉽게 진행이 가능하다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFP-RyouWgds",
        "outputId": "dd8cceee-e05e-4979-bde5-cb1712724f42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "다채로운 양말를(을) 만드는 회사에 어울리는 좋은 이름이 뭘까?\n",
            "What is a good name for a company that makes colorful socks?\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "## KO\n",
        "ko_prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"{product}를(을) 만드는 회사에 어울리는 좋은 이름이 뭘까?\"\n",
        ")\n",
        "print(ko_prompt.format(product=\"다채로운 양말\"))\n",
        "\n",
        "## EN\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\",\n",
        ")\n",
        "\n",
        "print(prompt.format(product=\"colorful socks\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEPwsEnvYmbA"
      },
      "source": [
        "## Chains: LLMs와 prompt들을 다중 스텝 워크플로우(multi-step workflow)에서 합쳐보자.\n",
        "지금까지는 PromptTemplate와 LLM을 단독으로 사용했었다. 하지만 실제 어플리케이션에서는 하나씩 단독으로 사용되는것이 아나라 이들의 조합으로 활용될 것이다.\n",
        "\n",
        "LangChain의 chain은 각각의 LLMs부터 또 다른 chain까지의 link들로 구성된다. chain의 가장 코어 타입은 LLMChain으로 PromptTemplate와 LLM으로 구성되어있다. 앞의 예시를 더욱 확징해 보면, user input을 받아 PromptTemplate을 통해 format시키고, 이를 LLM에 넣어주는 동작이 LLMChain 내부에서 일어날 것이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8HrYx1YaJWs",
        "outputId": "dde99f45-c072-41d0-bff4-11c7aa7db554"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Happy Socks & Co.\n",
            "\n",
            "\n",
            "1. 컬러풀한 양말 스타일\n",
            "2. 따뜻한 양말 특강\n",
            "3. 양말 스트릭트\n",
            "4. 다채로운 스타일 피트\n",
            "5. 신뢰할 수 있는 양말 세트\n",
            "6. 꾸며진 양말 전문가\n",
            "7. 누구나 소중하게 어울리는 양말 클라우드\n",
            "8. 지속적인 양말 디자인\n",
            "9. 따뜻\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = OpenAI(temperature=0.9)\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\"\n",
        ")\n",
        "\n",
        "## EN\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "print(chain.run(\"colorfulk socks\"))\n",
        "\n",
        "## KO\n",
        "ko_chain = LLMChain(llm=llm, prompt=ko_prompt)\n",
        "print(ko_chain.run(\"다채로운 양말\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGxlQlN6JCaj"
      },
      "source": [
        "## Agents: User의 input에 기반해 chain들을 동적으로 호출한다.\n",
        "Agent는 LLM을 취해야 할 action이나 어떠한 순서로 동작할지를 정하는데 사용한다. action은 tool을 사용해 output을 관찰하거나 user에게 반환해주거나 둘 중 하나일 수 있다.\n",
        "\n",
        "agents를 불러오기 위해 아래 concept들을 이해해 두어야 한다:\n",
        "\n",
        "\n",
        "* Tool : 특정한 과제를 수행할 함수. 이는 뒤 예시와 같을 수 있다: Google Search, Database lookup, Python REPL, 다른 chain들. tool의 interface는 현재 input:str, output:str인 function이다. \n",
        "* LLM : agent를 지원하는 언어 모델.\n",
        "* Agent : 사용될 agent. 이는 support agent class를 참조하는 문자열(str)이어야 한다. 이유는 해당 notebook(quickstart)는 간단하고, 높은 레벨의 API에 집중하고 있고, 이는 오직 기본적으로 지원되는 agent들만을 설명하고 있기 때문이다. custom agent를 만드는 doc은 추후 공개될 예정이다(현 23.05.08)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSbIrjNVMNAz",
        "outputId": "c16c50d3-1fa1-4e8b-e22d-89fd957f26cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32019 sha256=b26c61a8792db25c4ddc13c083639475e959386b3d867337ce3af0fef12ebc56\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install google-search-results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FY3RwBZMUJM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"...\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfa4_X9BQKUA"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# 첫쨰, agent를 컨트롤하는데 사용될 language model을 불러온다.\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# 그 다음, 사용할 몇가지 tool들을 불러온다. \"llm-math\" tool은 LLM을 사용하므로, llm도 같이 넣어주어야 한다.\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "# 마지막으로, agent를 tool, language model, agent의 type을 넣어 초기화한다.\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# 이제 테스트를 해보자!\n",
        "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG11wg5PSxHa"
      },
      "source": [
        "## Memory: Chain과 Agent에 상태(State)를 더해주자.\n",
        "chain과 agent가 이전 interaction에 대한 정보를 기억하는 \"memory\" concept를 가질 수 있게 한다. 가장 명확하고 간단한 예시는 chatvot을 디자인 할 때 인데, 챗봇의 더 좋은 발화능력을 위해 이전 대화를 저장해 둠으로써 맥락을 사용할 수 있게 만드는 것이다. \n",
        "\n",
        "LangChain은 위 목적에 부합하는 몇가지의 특정 chain들을 구현해 두었다. 이 notebook(quickstart)는 두 가지의 다른 타입의 memory에 대해 하나의 chain(ConversationChain)을 사용해 진행해 볼 것이다.\n",
        "* ConversationChain은 기본적으로 이전 모든 input/output을 기억하고 전달할 context에 이를 모두 더하는 식의 매우 간단한 타입의 memory를 가지고 있다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaXjYByXU3SM",
        "outputId": "c6382897-6209-475e-810f-226a3b17f840"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " Hi there! It's nice to meet you. My name is AI. What's your name?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: 반가워!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " 반가워! 나는 인공지능 챗봇이야. 나는 너에게 도움이 될 수 있을까?\n"
          ]
        }
      ],
      "source": [
        "from langchain import OpenAI, ConversationChain\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(llm=llm, verbose=True)\n",
        "\n",
        "## EN\n",
        "output = conversation.predict(input=\"Hi there!\")\n",
        "print(output)\n",
        "\n",
        "## EN\n",
        "ko_conversation = ConversationChain(llm=llm, verbose=True)\n",
        "ko_output = ko_conversation.predict(input=\"반가워!\")\n",
        "print(ko_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIA51-ofaFF3",
        "outputId": "bc7ae0bf-2bf0-4980-d140-a6ce75f4b43c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hi there! It's nice to meet you. My name is AI. What's your name?\n",
            "Human: I'm doing well! Just having a conversation with an AI.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: 반가워!\n",
            "AI:  반가워! 나는 인공지능 챗봇이야. 나는 너에게 도움이 될 수 있을까?\n",
            "Human: 잘 지내지! 어서 AI와 대화를 해보자.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " 좋아! 나는 너에게 도움이 될 수 있을까? 나는 너가 궁금한 것에 대해 답할 수 있어. 너는 무엇을 알고 싶어?\n"
          ]
        }
      ],
      "source": [
        "## EN\n",
        "output = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n",
        "print(output)\n",
        "\n",
        "## KO\n",
        "ko_output = ko_conversation.predict(input=\"잘 지내지! 어서 AI와 대화를 해보자.\")\n",
        "print(ko_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wPmMQpQbEXv"
      },
      "source": [
        "## Language Model 어플리케이션을 만들기: Chat Models\n",
        "LLM을 대신해서 chat model을 사용할 수 있다. Chat model들은 language model들의 변형체이다. Chat model들은 내부적으로 language model을 사용하지만, 이들이 노출하는 인터페이스는 약간 다르다: \"text in, text out\" API보다는, input output으로 \"chat message\"를 받는 interface를 노출한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6ZR3UlmcRIs"
      },
      "source": [
        "## Chat Model로부터 완성 Message 받기.\n",
        "완성된 chat을 얻기 위해서는 chat model에게 하나 이상의 message들을 보내주어야 한다. 이떄 응답은 message일 것이다. 현재 LangChain에서 지원되는 message들의 type은 AIMessage, HumanMessage, SystemMessage, 그리고 ChatMessage이다 - ChatMessage는 임의의 parameter를 받는다. 대부분의 경우, HumanMessage, AIMessage, SystemMessage만을 다루게 될 것이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QrtQgXIpdaOi"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cykt7APndo_L",
        "outputId": "54d11ed4-25bd-41ea-bfb5-877e300711f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='저는 프로그래밍을 좋아합니다.', additional_kwargs={}, example=False)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat([HumanMessage(content=\"Translate this sentence from English to Korean. I love programming.\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzeZHPvHdx_4",
        "outputId": "4201a77c-c42f-47ba-a90b-d9dd2be7f73d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='저는 프로그래밍을 좋아합니다.', additional_kwargs={}, example=False)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 여러개의 message들을 gpt-3.5-turbo또는 gpt-4 model에게 보낼 수 있다.\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates English to Korean.\"),\n",
        "    HumanMessage(content=\"I love programming.\")\n",
        "]\n",
        "\n",
        "chat(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYBK-QOZe7GX"
      },
      "source": [
        "한 단게 더 나아가 다중 message 세트들에 대해 generate 함수를 사용해 완성 message들을 생성할 수 있다. 이 결과는 LLMResult class일 것이며 추가적인 message parameter들이 담겨 있을 것이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzwPV-MEfOtw",
        "outputId": "9d1f48e4-f19b-4e9d-c8f1-437af64bd93b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LLMResult(generations=[[ChatGeneration(text='저는 프로그래밍을 좋아합니다.', generation_info=None, message=AIMessage(content='저는 프로그래밍을 좋아합니다.', additional_kwargs={}, example=False))], [ChatGeneration(text='저는 인공지능을 좋아합니다.', generation_info=None, message=AIMessage(content='저는 인공지능을 좋아합니다.', additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 27, 'total_tokens': 84}, 'model_name': 'gpt-3.5-turbo'})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_messages = [\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to Korean.\"),\n",
        "        HumanMessage(content=\"I love programming.\")\n",
        "    ],\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to Korean.\"),\n",
        "        HumanMessage(content=\"I love artificial intelligence.\")\n",
        "    ],\n",
        "]\n",
        "result = chat.generate(batch_messages)\n",
        "result"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
