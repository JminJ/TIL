# 공부한 것 #
* Transformer : Attention
* Transformer : Self-attetion
----------------------
## 정리 ##
* Attention
<img src = "images\2020_11_30_3.PNG">

> * Scaled Dot-Product Attention
> <img src = "images\2020_11_30_4.PNG">
>> query = Q, key = K, value = V, d_k = key의 dimension

> Scaled Dot-Prduct Attention은 query, key, value를 각각 Q, K, V라는 인자로 받아 연산을 수행한다. 
> 우선 query와 모든 key값을 dot product 계산을 한다. 나온 결과값들에 모두 √d_k값으로 나눠준다. 이후 softmax 함수를 적용시켜 value의 weights들을 구해낸다.
> **Matrix of outputs**
><img src = "images\2020_11_30_5.PNG">
> 
> * √d_k로 나눠주는 이유는 dot-product 값이 커질 수록 softmax 함수에서 기울기의 변화가 거의 없는 부분으로 가기 때문이다.
> * attention의 원리 처럼 softmax를 거친 값에 value를 곱하면 query와 유사한 value일수록(중요한 value) 더 높은 값을 가지게 된다.

 
 >* Multi-Head Attention 
 > <img src = "images\2020_11_30_6.PNG">
 >
 >  다시 한번 설명하자면 Multi-Head Attention은 Scaled Dot-Pruduct Attention을 h번 쌓은 레이어다.  즉 동일한 Q, K, V에 각각 다른 weight matrix W를 곱해주는 것이다. Scaled Dot-Product를 d_h dimension의 key, value, query들로 한 번 실행하지 않고 각각 h번 key, value, query에 다른 학습된 linear projection을 수행한 이유는 이 방식이 더욱 성능이 좋고 병렬구조로 한번에 학습이 가능하다.
> <img src = "images\2020_11_30_7.PNG">

* Self-Attention
> * Encoder self-attention layer
> <img src ="images\2020_12_02_4.PNG">
>
> Encoder의 multi-head attention layer에서 입력값은 Q, K, V이고 이들은 모두 같은 값을 가진다. 또한 모두 이전 Encoder의 output을 가지고 오므로 이전 layer의 모든 position에 attention을 적용해 줄 수 있게 된다. 만약 첫 번째 layer라면 positionnal encoding이 더해진 input embedding이 된다.

> * Decoder self-attention layer
>
><img src = "images\2020_12_02_3.PNG">
>
> Encoder의 multi-head attention과는 다르게 masking out을 해준 multi-head attention을 사용하였다. masking out이 됐다는 말은 i번째 position에 대한 attention을 얻을 때 i번째 이후에 있는 모든 position을 input 값을 -∞으로 설정한 것이다. 이렇게 하면 i번째  position 이후의 모든 positoin에 attention을 주는 경우가 없을 것이다.
>
> **Masking 그림**
> <img src = "images\2020_12_02_2.PNG">
>
> masking out을 적용해주는 이유 : i번째 output을 다시 i+1번째 input으로 사용하는 auto-regressive한 특성을 유지시키기 위해 masking out을 적용시켜 주었다.

> * Encoder-Decoder Attention Layer
>
><img src = "images\2020_12_02_5.PNG">
>
> query들은 masked multi-head attention에서 받고 key, value들은 Encoder에서 받아오게 된다. 그래서 Decoder의 모든 position에서 Encoder output의 모든 position에 attention을 줄 수 있게 된다.
---------------------------
## Notion Link ##
* https://www.notion.so/Attention-is-all-you-need-acaa784fe3864d69a21710401f35d2b7
-------------------------
## 출처 ##
* https://pozalabs.github.io/transformer/