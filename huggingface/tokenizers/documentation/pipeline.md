# pipeline
-----
## normalizers

```python
from toknizers import normalizers
from tokenizers.normalizers import NFD, StripAccents

normalizer = normalizers.Sequence([NFD(), StripAccents()])
```

- normalizersëŠ” ë§ ê·¸ëŒ€ë¡œ raw dataë¥¼ normalization í•´ì£¼ëŠ” í•¨ìˆ˜ì´ë‹¤. Sequence()ë¥¼ í†µí•´ ì—¬ëŸ¬ ê°œì˜ normalization í•¨ìˆ˜ë¥¼ ë™ì‹œì— ì ìš©ì´ ê°€ëŠ¥í•˜ë‹¤.

```python
## normalizer ì˜ˆì œ
normalizer.normalize_str("HÃ©llÃ² hÃ´w are Ã¼?")
# "Hello how are u?"

## tokenizerì˜ normalizer êµì²´ 
tokenizer.normalizer = normalizer
```

## Pre-tokenization

- textë¥¼ wordë‹¨ìœ„ë¡œ ëŠì–´ ì–´ëŠ ë¶€ë¶„ì´ trainingì˜ ëì¸ì§€ë¥¼ ì•Œ ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë°©ë²•ì´ë‹¤.
- (ì˜ì–´ ê¸°ì¤€) ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì€ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ëŠëŠ” ë°©ë²•ì´ë‹¤.

```python
from tokenizers.pre_tokenizers import Whitespace

pre_tokenizer = Whitespace()
pre_tokenizer.pre_tokenize_str("Hello! How are you? I'm fine, thank you.")
# [("Hello", (0, 5)), ("!", (5, 6)), ("How", (7, 10)), ("are", (11, 14)), ("you", (15, 18)),
#  ("?", (18, 19)), ("I", (20, 21)), ("'", (21, 22)), ('m', (22, 23)), ("fine", (24, 28)),
#  (",", (28, 29)), ("thank", (30, 35)), ("you", (36, 39)), (".", (39, 40))]
```

- ì—¬ëŸ¬ê°œì˜ Pre-tokenizerë¥¼ í•©ì³ ì‚¬ìš©í•˜ëŠ” ê²ƒ ë˜í•œ ê°€ëŠ¥í•˜ë‹¤.

```python
from tokenizers import pre_tokenizers
from tokenizers.pre_tokenizers import Digits

pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])
pre_tokenizer.pre_tokenize_str("Call 911!")
# [("Call", (0, 4)), ("9", (5, 6)), ("1", (6, 7)), ("1", (7, 8)), ("!", (8, 9))]
```

- ìœ„ë¦ normalizersì²˜ëŸ¼ tokenizerì˜ pre-tokenizerë¥¼ ë°”ê¿€ ìˆ˜ ìˆë‹¤.

```python
tokenizer.pre_tokenizer = pre_tokenizer
```

## The-model

- tokenizerê°€ normalizeì™€ pre-tokenizationì„ ìˆ˜í–‰í–ˆë‹¤ë©´ ì´ì œ pre-tokenì— modelì„ ì ìš©í•´ì•¼ í•œë‹¤.
- ì´ modelì˜ roleì€ wordë¥¼ ë¯¸ë¦¬ í•™ìŠµëœ ë°©ë²•ìœ¼ë¡œ "token"ì˜ í˜•íƒœë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì´ë‹¤. ë‚˜ëˆ ì§„ tokenë“¤ì€ model ë‚´ì˜ vocabularyì— ìˆëŠ” ë™ì¼í•œ tokenì˜ IDë“¤ë¡œ mappingëœë‹¤.
- ğŸ¤— tokenizerì—ì„œ ì§€ì›í•˜ëŠ” modelë“¤ì€ ì•„ë˜ì™€ ê°™ë‹¤.

```
* BPE
* WordLevel
* Unigram
* WordPiece
```

## Post-Processing

- post-processing ì‘ì—…ì€ tokenizerì˜ ë§ˆì§€ë§‰ ì‘ì—…ì´ë‹¤. return ë˜ê¸° ì „ì— ì¶”ê°€ì ì¸ tokenì„ ì¶”ê°€í•˜ëŠ” ë“± transformation ì‘ì—…ì„ ë” í•´ì£¼ëŠ” ê²ƒì´ë‹¤.

```python
from tokenizers.processors import TemplateProcessing

tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[("[CLS]", 1), ("[SEP]", 2)],
)
```

- ìœ„ ì˜ˆì‹œëŠ” BERTì— ë“¤ì–´ê°€ëŠ” inputì— ë§ê²Œ post-processing ì‘ì—…ì„ í•´ì¤€ ê²ƒì´ë‹¤.

---

### ì¶œì²˜

- [https://huggingface.co/docs/tokenizers/python/latest/pipeline.html](https://huggingface.co/docs/tokenizers/python/latest/pipeline.html)