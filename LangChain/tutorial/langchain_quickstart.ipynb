{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNeY8CkNUO7B",
        "outputId": "99529c6b-dd7a-453e-d552-2d4070973559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat May  6 13:48:01 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqgeJeVFUfFf"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lajVXpfBUecL",
        "outputId": "640e69d1-4bea-41ad-90be-fa1cd2519c07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Using cached langchain-0.0.160-py3-none-any.whl (756 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2\n",
            "  Using cached openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7\n",
            "  Using cached dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Collecting async-timeout<5.0.0,>=4.0.0\n",
            "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3\n",
            "  Using cached aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.65.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Using cached frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Using cached yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Collecting typing-inspect>=0.4.0\n",
            "  Using cached typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0\n",
            "  Using cached marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "Collecting marshmallow-enum<2.0.0,>=1.5.1\n",
            "  Using cached marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, openapi-schema-pydantic, marshmallow-enum, aiosignal, dataclasses-json, aiohttp, langchain\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.7 frozenlist-1.3.3 langchain-0.0.160 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.8.0 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.6\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Yej_dI-MVHU-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"]=\"...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWB4cfKvVdCI"
      },
      "source": [
        "## LLMs: language model로부터 prediction들을 얻자.\n",
        "가장 기본적인 LangChain의 building block은 input에 대해 LLM을 불러 내는 것이다. 해당 코드의 목적은 회사가 무슨 일을 하는가에 따라 회사명을 생성해 보는 것이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-644N3RV-_0",
        "outputId": "81bf1d1d-00b9-4218-e7c2-73b9aa24e722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "★열정같이 따뜻해지는 양말★\n",
            "\n",
            "\n",
            "Fuzzy Feet Toesies.\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Random성을 가중시키기 위해 temperature를 높게 설정해본다.\n",
        "llm = OpenAI(temperature=0.9)\n",
        "\n",
        "## KO\n",
        "text = \"다채로운 양말을 만드는 회사에 어울리는 회사명은 무엇일까?\"\n",
        "print(llm(text))\n",
        "\n",
        "## EN\n",
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "print(llm(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtn69_P4W2fg"
      },
      "source": [
        "## Prompt Templates: LLMs를 위한 prompt들을 관리한다.\n",
        "일반적으로 LLM이 어플리케이션에서 사용될 떄, 유저의 input을 그대로 LLM에 넣지는 않을 것이다.\n",
        "대신에, 아마도 당신은 유저의 input을 받고 prompt를 구성한 다음 LLM에게 그것을 전달해 줄 것이다.\n",
        "\n",
        "위 예시를 업데이트 해본다면, 우리는 유저에게 회사가 하는 일만을 입력 받고 그 정보들로 prompt의 format을 맞춰주는 것을 생각해 볼 수 있겠다. 이 작업은 LangChain을 통해서면 매우 쉽게 진행이 가능하다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFP-RyouWgds",
        "outputId": "dd8cceee-e05e-4979-bde5-cb1712724f42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "다채로운 양말를(을) 만드는 회사에 어울리는 좋은 이름이 뭘까?\n",
            "What is a good name for a company that makes colorful socks?\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "## KO\n",
        "ko_prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"{product}를(을) 만드는 회사에 어울리는 좋은 이름이 뭘까?\"\n",
        ")\n",
        "print(ko_prompt.format(product=\"다채로운 양말\"))\n",
        "\n",
        "## EN\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\",\n",
        ")\n",
        "\n",
        "print(prompt.format(product=\"colorful socks\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEPwsEnvYmbA"
      },
      "source": [
        "## Chains: LLMs와 prompt들을 다중 스텝 워크플로우(multi-step workflow)에서 합쳐보자.\n",
        "지금까지는 PromptTemplate와 LLM을 단독으로 사용했었다. 하지만 실제 어플리케이션에서는 하나씩 단독으로 사용되는것이 아나라 이들의 조합으로 활용될 것이다.\n",
        "\n",
        "LangChain의 chain은 각각의 LLMs부터 또 다른 chain까지의 link들로 구성된다. chain의 가장 코어 타입은 LLMChain으로 PromptTemplate와 LLM으로 구성되어있다. 앞의 예시를 더욱 확징해 보면, user input을 받아 PromptTemplate을 통해 format시키고, 이를 LLM에 넣어주는 동작이 LLMChain 내부에서 일어날 것이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8HrYx1YaJWs",
        "outputId": "dde99f45-c072-41d0-bff4-11c7aa7db554"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Happy Socks & Co.\n",
            "\n",
            "\n",
            "1. 컬러풀한 양말 스타일\n",
            "2. 따뜻한 양말 특강\n",
            "3. 양말 스트릭트\n",
            "4. 다채로운 스타일 피트\n",
            "5. 신뢰할 수 있는 양말 세트\n",
            "6. 꾸며진 양말 전문가\n",
            "7. 누구나 소중하게 어울리는 양말 클라우드\n",
            "8. 지속적인 양말 디자인\n",
            "9. 따뜻\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = OpenAI(temperature=0.9)\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\"\n",
        ")\n",
        "\n",
        "## EN\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "print(chain.run(\"colorfulk socks\"))\n",
        "\n",
        "## KO\n",
        "ko_chain = LLMChain(llm=llm, prompt=ko_prompt)\n",
        "print(ko_chain.run(\"다채로운 양말\"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
