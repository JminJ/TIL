# 공부한 것 #
* Transformer : Position-wise Feed-Forward Networks
* Transformer : Embeddings ans Softmax
* Transformer : Positional Encoding
------------------------
## 정리 ##
* Position-wise Feed-Forward Networks
> Encoder와 Decoder의 모든 layer들은 fully connected feed-forward network를 포함하고 있다. 이는 각 position마다 적용되기 때문에 position-wise라고 불린다 (각 position마다 적용된다는 것은 각 단어마다 적용된다는 뜻).

<img src = "images\2020_12_02_2.PNG">    

> Position-wise Feed-Forward Networks는 두 번의 linear transformation과 한 번의 activation function ReLU로 이루어져 있다.
> * 각 positoin마다 같은 parameter를 사용한다고 했으나 layer마다는 서로 다른 parameter를 사용해 학습한다. 그리고 이는 kernel size가 1인 Convolution Layer를 두 번 사용한 것과 같이 설명할 수 있다.

* Embeddings and Softmax
> input token과 output token을 dimension이 d_model인 vector로 만들어 주기 위해 learned embedding을 사용한다. 또한 Decoder output를 다음 token의 확률로 바꾸기 위해서도 사용했다.

* Positional Encoding
> recurrence도 convolution도 아니기 때문에 단어의 순서에 대한 상대적 또는 절대적 위치에 대한 정보를 더해주어야 한다. 따라서 Encoder와 Decoder의 input embedding에 Positonal Encoding을 더해주기로 했다.
> * Positional Encoding은 d_model과 같은 차원을 가지고 있으므로 Embedding vector와 더해질 수 있다.

> 이 작업 과정에서 각각 다른 과정의 sine, cosine function을 사용하게 된다.

<img src = "images\2020_12_02_1.PNG">

> * pos : position을 뜻한다
> * i : dimension(차원)을 뜻한다.

> 추가적으로 학습된 positional embedding 대신 sinusoidal version을 택한 이유는 positional embedding 같은 경우에는 training보다 더욱 긴 sequence가 inference시에 입력으로 들어올 때 문제가 발생하지만 **sinusoidal의 경우에는 const하기에 문제가 되지 않는다. 단지 계산을 더 많이 하기만 하면 될 뿐**이다.
--------------------
## Notion Link ##
* https://www.notion.so/Attention-is-all-you-need-acaa784fe3864d69a21710401f35d2b7
--------------------
## 참고 ##
* https://pozalabs.github.io/transformer/