# 공부한 것 # 
* Transformer - Encoder & Decoder
----------------
## 정리 ##
 Transformer 모델은 기존의 RNN/CNN 기반으로 구성된 모델들의 단점인 계산량이 너무 많다는 문제점을 해결하기 위해 이 방식에서 벗어나 self-attention만을 사용해 학습을 한다는 점이 특별하다.
 <img src = "images\2020_11_30_1.PNG">
 * n, d, k, r 설명
 > n : sequence의 길이
 > d : 표현 차원
 > k : kenel 사이즈
 > r : neiborhood의 수

 기존 방식을 사용하는 모델들보다 얼마나 좋은 성과를 낼 수 있는지 의문이 생길 수 있다. 위 표에서 보여주듯 계산복잡도에서 훨씬 좋은 모습을 보이며 네트워크의 긴 거리 의존성이 있는 단어 사이의 path 길이면에서 유리하다.
 <img src = "images\2020_11_30_2.PNG">
 * Encoder, Decoder
 > Encoder는 Figure 1의 왼쪽 부분이다.
 > Decoder는 Figure 1의 오른쪽 부분이다.

 ### Encoder ###
 * Encoder는 모델에서 처리하는 모든 기본 입력 차원을 512로 지정하였다 (Residual connection을 편하게 하기 위함).  
 * 구성은 sub-layer 두 개와, multi-head self-attention,  position-wise fully connected feed-forward network로 이루어져 있다. 
 * 구성은 sub-layer 두 개와, multi-head self-attention,  position-wise fully connected feed-forward network로 이루어져 있다. 

 ### Decoder ###
 * Encoder처럼 multi-head self-attention, position-wise fully connected feed-forward network를 가지고 있다. 하지만 multi-head self-connection을 수행할 때 Encoder의 결과와 Decoder의 결과값과 함께 계산을 해준다.
 * Encoder과 다르게 masked multi-head self-attention을 진행한다. 이를 통해 position i보다 이후의 position에 대해 attention을 주지 못하게 해준다.
 * sub-layer에 residual connection과 layer normalization을 수행한다.
---------------
## Notion Link ##
* Transformer : 
<https://www.notion.so/Transformer-03c0bd1741334c2b8d8e900970c67777>
--------
### 참고 ###
* https://pozalabs.github.io/transformer/
* https://dalpo0814.tistory.com/49
